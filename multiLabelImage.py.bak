import torch
import torch.optim as optim
import torch.utils.data
import torch.backends.cudnn as cudnn
import torchvision
from torchvision import transforms, datasets
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
import pandas as pd
from torchvision.io import read_image

#--- hyperparameters ---
N_EPOCHS = 30   
BATCH_SIZE_TRAIN = 100
BATCH_SIZE_TEST = 100
LR = 0.1#0.01


# torch.nn.functional.one_hot(torch.arange(0,5),10) 
#--- fixed constants ---
NUM_CLASSES = 14
NUM_IMAGES = 20000
img_dir = '../data/images'

#annotations_file = '../data/annotations/baby.txt'

import os
import pandas as pd
from torchvision.io import read_image


#--- get images and make multi-hot-matrix ---
label_names = ['baby','bird','car','clouds','dog','female','flower','male','night','people','portrait','river','sea','tree']
one_hot = np.zeros([20000,NUM_CLASSES],dtype=int)
for c in range(0, NUM_CLASSES):
    annotations_file = '../data/annotations/' + label_names[c] + '.txt'            
    one_hot[np.loadtxt(annotations_file,dtype=int)-1,c] = 1 #  IS -1 NECESSARY??
#labels = np.loadtxt(annotations_file,dtype=int)-1

# create weights, used in lossfunction (maybe not useful?)
label_counts = sum(one_hot)
#WEIGHTS = torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
WEIGHTS = torch.from_numpy(label_counts/NUM_IMAGES)



#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files
class CustomImageDataset(torch.utils.data.Dataset):
    def __init__(self, label_names, img_dir, transform=transforms.Grayscale(), target_transform=None):
        one_hot = np.zeros([20000,NUM_CLASSES],dtype=int)
        for c in range(0, NUM_CLASSES):
            annotations_file = '../data/annotations/' + label_names[c] + '.txt'            
            one_hot[np.loadtxt(annotations_file,dtype=int)-1,c] = 1 #  IS -1 NECESSARY??
        
        self.img_labels = one_hot
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        #img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) # when transfomrmed to dataframe with multiple columns..?
        im_name = 'im%s.jpg' % str(idx+1)
        img_path = os.path.join(self.img_dir, im_name)
        image = read_image(img_path)
        
        #label = self.img_labels.iloc[idx, 1]
        label = self.img_labels[idx]
        if self.transform:
            if torch.Tensor.size(image,0) == 3:
                image = self.transform(image)
           
        if self.target_transform:
            label = self.target_transform(label)
        return image, label


mydata = CustomImageDataset(label_names,img_dir)
# --- Dataset initialization ---
[train_set, dev_set] = torch.utils.data.random_split(mydata, [int(NUM_IMAGES*0.5), int(NUM_IMAGES*0.5)],generator=torch.Generator().manual_seed(42))

## reduce imbalance in training (only), show more images with rare labels
train_onehot = one_hot[train_set.indices]
label_counts_train = sum(train_onehot)
STIM_WEIGHT = train_onehot*(1./label_counts_train)*1000
STIM_WEIGHT = np.sum(STIM_WEIGHT,axis=1)
STIM_WEIGHT = torch.from_numpy(STIM_WEIGHT)+ 0.5 #some images have no labels and would have value 0? would it matter?
sampler = torch.utils.data.WeightedRandomSampler(STIM_WEIGHT, len(STIM_WEIGHT))


#train_loader = torch.utils.data.DataLoader(dataset=mydata, batch_size=BATCH_SIZE_TRAIN, shuffle=False)
train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=False,sampler=sampler)
#test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)
dev_loader = torch.utils.data.DataLoader(dataset=dev_set, batch_size=BATCH_SIZE_TEST, shuffle=True)



class CNN(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(CNN, self).__init__()
        self.layer1 = nn.Sequential(
          nn.Conv2d(1,40,3),
          torch.nn.InstanceNorm2d(32),
          nn.ReLU(), 
          torch.nn.Dropout(p=0.35, inplace=False),
          nn.MaxPool2d(2,None),
          
          # three layer layers
          nn.Conv2d(40,24,5, padding=1),
          torch.nn.InstanceNorm2d(32),
          nn.ReLU(), 
          torch.nn.Dropout(p=0.35, inplace=False),
          nn.MaxPool2d(2,None),
          
          nn.Conv2d(24,10,9,padding=3),
          torch.nn.InstanceNorm2d(32),
          nn.ReLU(), 
          torch.nn.Dropout(p=0.35, inplace=False),
          nn.MaxPool2d(2,None)
          
          # two layers
          #nn.Conv2d(40,5,15,padding=3),
          #torch.nn.InstanceNorm2d(32),
          #nn.ReLU(), 
          #torch.nn.Dropout(p=0.35, inplace=False),
          #nn.MaxPool2d(2,None)
          
          
        )
        self.layer2 = nn.Sequential(
            nn.Linear(1960,14), 
            #nn.Linear(3645,14), 
            nn.Sigmoid()
            #torch.nn.Dropout(p=0.35, inplace=False)
        )
        
    def forward(self, x):
        x = self.layer1(x)
        x = torch.flatten(x, 1)
        x = self.layer2(x)
        
        return x
        
#--- set up ---
if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

model = CNN().to(device)

#optimizer = optim.SGD(model.parameters(), lr=LR)
optimizer = optim.SGD(model.parameters(), lr=LR,momentum=0.9)
#optimizer = optim.Adam(model.parameters(), lr=LR)
loss_function = nn.BCEWithLogitsLoss(weight=WEIGHTS) #nn.MultiLabelSoftMarginLoss(reduction='none') #nn.BCEWithLogitsLoss()
dev_accuracies = np.zeros([1,N_EPOCHS])

#--- training ---
for epoch in range(N_EPOCHS):
    train_loss = 0
    train_correct = 0
    total = 0
    chance_correct = 0
    falsePositives = torch.zeros(14)
    falseNegatives = torch.zeros(14)
    for batch_num, (data, target) in enumerate(train_loader):
        if batch_num > 10:
            break
        
        model.zero_grad()
        data, target = data.to(device), target.to(device)
        #target = target.byte()
        data = data.float() # THIS SHOULD BE MOVED TO INIT SOMEHOW, SOLVES THE 
        # PROBLEM OF EXPECTED TYPE BYTE/FLOAT PROBLEM
        prediction = model(data)
        #prediction = prediction * WEIGHTS # should this be here before loss?...
        #target = target.long()
        target = target.float()
        loss = loss_function(prediction, target)
        # l2 regularization
        l2_lambda = 0.001
        l2_norm = sum(p.pow(2.0).sum()
                 for p in model.parameters())
        loss = loss + l2_lambda * l2_norm
        
        train_loss = train_loss + loss
        loss.backward()
        optimizer.step()
        
        #_, predLabel = torch.max(prediction, 1)
        total += target.size(0)*target.size(1)
 
        #prediction = (prediction * 0.7).round() # weight non-labels, but only affect accuracy, not model itself...
        prediction = prediction.round()
        train_correct += (prediction == target).sum().item()
        
        # false positive and negative rates to see whether predicts only zeros
        falsePosNeg = prediction - target
        falsePos = falsePosNeg > 0 
        falseNeg = falsePosNeg < 0 
        falsePositives += torch.count_nonzero(falsePos,dim=0)
        falseNegatives += torch.count_nonzero(falseNeg,dim=0)
        chance_correct += torch.count_nonzero(target-1).item()
        print('target amounts %d' % target.sum())
        
        correct_over_chance = train_correct - chance_correct
        
            
        if batch_num < 1000: #== len(train_loader)-1:
            print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Train Acc: %.3f%% (%d/%d)' % 
              (epoch, batch_num+1, len(train_loader), train_loss / (batch_num + 1), 
               100. * correct_over_chance / total, train_correct, total))
            #print('False Positives, false negatives %')
            #print(100. * falsePositives / (total / NUM_CLASSES))
            #print(100. * falseNegatives / (total / NUM_CLASSES))
            print('FPs: %.3f%%  FN:s: %.3f%% ' % ((100. * falsePositives.sum() / total).item(),
                  (100. * falseNegatives.sum() / total).item()))
            
    
    
    # DEV TEST
    all_dev_acc = []
    dev_correct = 0
    dev_total = 0
    chance_correct_dev = 0
    falsePositives = torch.zeros(14)
    falseNegatives = torch.zeros(14)
    for dev_batch_num, (dev_data, dev_target) in enumerate(dev_loader):
        if dev_batch_num > 10:
            break
        dev_data, dev_target = dev_data.to(device), dev_target.to(device)
        dev_data = dev_data.float()
        devPrediction = model.forward(dev_data)
        dev_target = dev_target.float()
        #_, devLabel = torch.max(devPrediction,1)

        devPrediction = devPrediction.round()
        dev_total += dev_target.size(0)*dev_target.size(1)
        dev_correct += (devPrediction == dev_target).sum().item()
        
        falsePosNeg = devPrediction - dev_target
        falsePos = falsePosNeg > 0 
        falseNeg = falsePosNeg < 0 
        falsePositives += torch.count_nonzero(falsePos,dim=0)
        falseNegatives += torch.count_nonzero(falseNeg,dim=0)
        chance_correct_dev += torch.count_nonzero(dev_target-1).item()
        print('target amounts dev %d' % target.sum())
        
        correct_over_chance = dev_correct - chance_correct_dev
        
        #if epoch =
        if dev_batch_num < 1000: #== len(dev_loader)-1:
            print('Dev test: Epoch %d - Batch %d/%d: Dev Acc: %.3f%% (%d/%d)' % 
              (epoch, dev_batch_num+1, len(dev_loader), 
               100. * correct_over_chance / dev_total, dev_correct, dev_total))
            #print('False Positives, false negatives %')
            #print(100. * falsePositives / (total / NUM_CLASSES))
            #print(100. * falseNegatives / (total / NUM_CLASSES))
            print('FPs: %.3f%%  FN:s: %.3f%% ' % ((100. * falsePositives.sum() / dev_total).item(),
                  (100. * falseNegatives.sum() / dev_total).item()))
            
    dev_accuracies[0,epoch] = dev_correct / dev_total



